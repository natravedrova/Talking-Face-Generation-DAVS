# Heavily relies on the following code:
# https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/
import numpy as np
import cv2
import dlib

TRAINED_SHAPE_PREDICTOR = '../preprocess/shape_predictor_68_face_landmarks.dat'


# TODO: tune usage of face landmark as good features for track
def find_face_landmarks(img, predictor):
    detector = dlib.get_frontal_face_detector()
    dets = detector(img, 1)
    try:
        shape = predictor(img, dets[0])
    except IndexError:
        print("No face landmarks were found.")
        return np.nan
    return list(map(lambda p: np.matrix([p.x, p.y]), shape.parts()))


def get_three_face_coordinates_from_68_face_landmarks(landmarks):
    """
    The key points for face alignment we used are the two for the center of the eyes and
    the average point of the corners of the mouth
    :param landmarks: list of numpy matrices returned by find_face_landmarks function
    :return: list of three lists with required coordinates
    """
    left_eye = np.mean(landmarks[36:42], axis=0)
    right_eye = np.mean(landmarks[42:48], axis=0)
    mouth = np.mean(landmarks[48:], axis=0)
    return np.array([left_eye, right_eye, mouth]).astype(np.float32)


def moving_average(curve, radius):
    """
    Moving average filter that takes in any curve (i.e. a 1-D of numbers) as an input and
    returns the smoothed version of the curve
    """
    window_size = 2 * radius + 1
    # Define the filter
    f = np.ones(window_size)/window_size
    # Add padding to the boundaries
    curve_pad = np.lib.pad(curve, (radius, radius), 'edge')
    # Apply convolution
    curve_smoothed = np.convolve(curve_pad, f, mode='same')
    # Remove padding
    curve_smoothed = curve_smoothed[radius:-radius]
    # return smoothed curve
    return curve_smoothed


def fix_border(frame):
    """
    When we stabilize a video, we may see some black boundary artifacts. This is expected because to stabilize the
    video, a frame may have to shrink in size.
    We can mitigate the problem by scaling the video about its center by a small amount (e.g. 4%).

    This function shows the implementation. We use getRotationMatrix2D because it scales and rotates the image without
    moving the center of the image. All we need to do is call this function with 0 rotation and
    scale 1.04 ( i.e. 4% upscale).
    """
    s = frame.shape
    # Scale the image 4% without moving the center
    T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)
    frame = cv2.warpAffine(frame, T, (s[1], s[0]))
    return frame


def smooth(trajectory, smoothing_radius=30):
    """
    Takes in the trajectory and performs smoothing on the three components
    smoothing_radius is in frames. The larger the more stable the video, but less reactive to sudden panning
    """
    smoothed_trajectory = np.copy(trajectory)
    # Filter the x, y and angle curves
    for i in range(3):
        smoothed_trajectory[:, i] = moving_average(trajectory[:, i], radius=smoothing_radius)
    return smoothed_trajectory


def stabilize_video(input_path, output_path):
    """
    Stabilize video generated by the network
    :param input_path: full path to input video file
    :param output_path: full path to save processed video
    :return: None
    """
    cap = cv2.VideoCapture(input_path)
    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    # Get width and height of video stream
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    # Define the codec for output video
    fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # for avi output; http://www.fourcc.org/codecs.php
    # Get frames per second (fps)
    fps = cap.get(cv2.CAP_PROP_FPS)
    out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))

    # Read first frame as a grayscale image
    _, prev = cap.read()
    prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)

    predictor = dlib.shape_predictor(TRAINED_SHAPE_PREDICTOR)
    # Pre-define transformation-store array
    transforms = np.zeros((n_frames-1, 3), np.float32)
    for i in range(n_frames-2):
        # Detect feature points in previous frame
        # landmarks = find_face_landmarks(prev_gray, predictor)
        # prev_pts = get_three_face_coordinates_from_68_face_landmarks(landmarks)
        prev_pts = cv2.goodFeaturesToTrack(prev_gray,
                                           maxCorners=30,
                                           qualityLevel=0.01,
                                           minDistance=5,
                                           blockSize=3)
        success, curr = cap.read()
        if not success:
            break
        curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)
        # Calculate optical flow (i.e. track feature points)
        curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None)
        assert len(prev_pts) == len(curr_pts)

        # Filter only valid points
        idx = np.where(status == 1)[0]
        prev_pts = prev_pts[idx]
        curr_pts = curr_pts[idx]
        # Find transformation matrix
        m = cv2.estimateRigidTransform(prev_pts, curr_pts, fullAffine=False)  # will only work with OpenCV-3 or less
        # Extract translation
        dx = m[0, 2]
        dy = m[1, 2]
        # Extract rotation angle
        da = np.arctan2(m[1, 0], m[0, 0])
        # Store transformation
        transforms[i] = [dx, dy, da]
        # Move to next frame
        prev_gray = curr_gray
        # print("Frame: " + str(i) + "/" + str(n_frames) + " -  Tracked points : " + str(len(prev_pts)))

    # Compute trajectory using cumulative sum of transformations
    trajectory = np.cumsum(transforms, axis=0)
    smoothed_trajectory = smooth(trajectory)
    # Calculate difference in smoothed_trajectory and trajectory
    difference = smoothed_trajectory - trajectory
    # Calculate newer transformation array
    transforms_smooth = transforms + difference

    ##################################################################
    # Loop over the frames and apply the transforms we just calculated
    ###################################################################
    # Reset stream to first frame
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # Write n_frames-1 transformed frames
    for i in range(n_frames - 2):
        # Read next frame
        success, frame = cap.read()
        if not success:
            break

        # Extract transformations from the new transformation array
        dx = transforms_smooth[i, 0]
        dy = transforms_smooth[i, 1]
        da = transforms_smooth[i, 2]

        # Reconstruct transformation matrix accordingly to new values
        m = np.zeros((2, 3), np.float32)
        m[0, 0] = np.cos(da)
        m[0, 1] = -np.sin(da)
        m[1, 0] = np.sin(da)
        m[1, 1] = np.cos(da)
        m[0, 2] = dx
        m[1, 2] = dy

        # Apply affine wrapping to the given frame
        frame_stabilized = cv2.warpAffine(frame, m, (w, h))

        # Fix border artifacts
        frame_stabilized = fix_border(frame_stabilized)

        # Write the frame to the file
        frame_out = cv2.hconcat([frame, frame_stabilized])

        cv2.imshow("Before and After", frame_out)
        cv2.waitKey(50)
        out.write(frame_stabilized)

    return


def main():
    stabilize_video('../output2.mp4', '../ouput2_stable.mov')


if __name__ == '__main__':
    main()
